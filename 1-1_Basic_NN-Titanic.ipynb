{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1-1 - Basic Neural Network - Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we simply iterated through randomly-generated matrices and chose the best-performing one. We build on this approach by reducing loss in a systematic way via stochastic gradient descent. In particular, we'll be using TensorFlow, an open source library developed by Google, and Keras, a high-level wrapper on top of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "df = pd.read_csv('data/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df.iloc[:712, :]\n",
    "\n",
    "df_train = df_train.drop(['Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)\n",
    "\n",
    "age_mean = df_train['Age'].mean()\n",
    "df_train['Age'] = df_train['Age'].fillna(age_mean)\n",
    "df_train['Sex'] = df_train['Sex'].map({'female': 0, 'male': 1}).astype(int)\n",
    "\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(df_train[features].values)\n",
    "y_train = df_train['Survived'].values\n",
    "y_train_onehot = pd.get_dummies(df_train['Survived']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = df.iloc[712:, :]\n",
    "\n",
    "df_test = df_test.drop(['Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)\n",
    "\n",
    "df_test['Age'] = df_test['Age'].fillna(age_mean)\n",
    "df_test['Sex'] = df_test['Sex'].map({'female': 0, 'male': 1}).astype(int)\n",
    "\n",
    "X_test = scaler.transform(df_test[features].values)\n",
    "y_test = df_test['Survived'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "\n",
      "accuracy 0.832402234637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=0, verbose=3)\n",
    "model = model.fit(X_train, df_train['Survived'].values)\n",
    "\n",
    "y_prediction = model.predict(X_test)\n",
    "print \"\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of generating a linear stack of layers with Numpy, we'll be implementing our model using Keras. We initialize our model, add a layer that inputs vectors of length 6 and outputs vectors of length 2, and finally add a softmax layer. We configure the learning process in the compilation step by specifying the optimizer, loss function and performance metric.\n",
    "\n",
    "Stochastic gradient descent acts by changing the weights gradually in the 'direction' that decreases the average loss. In other words, a particular weight would be increased if acts to decrease loss, or the weight decreased if it acts to increase loss. TensorFlow does the heavy-lifting by efficiently handling these numerical computations under the hood. A simple example of gradient descent is illustrated in the Appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "712/712 [==============================] - 0s - loss: 0.6828 - acc: 0.7191     \n",
      "Epoch 2/10\n",
      "712/712 [==============================] - 0s - loss: 0.6620 - acc: 0.7233     \n",
      "Epoch 3/10\n",
      "712/712 [==============================] - 0s - loss: 0.6441 - acc: 0.7317     \n",
      "Epoch 4/10\n",
      "712/712 [==============================] - 0s - loss: 0.6266 - acc: 0.7360     \n",
      "Epoch 5/10\n",
      "712/712 [==============================] - 0s - loss: 0.6108 - acc: 0.7444     \n",
      "Epoch 6/10\n",
      "712/712 [==============================] - 0s - loss: 0.5967 - acc: 0.7472     \n",
      "Epoch 7/10\n",
      "712/712 [==============================] - 0s - loss: 0.5840 - acc: 0.7584     \n",
      "Epoch 8/10\n",
      "712/712 [==============================] - 0s - loss: 0.5732 - acc: 0.7626     \n",
      "Epoch 9/10\n",
      "712/712 [==============================] - 0s - loss: 0.5623 - acc: 0.7640     \n",
      "Epoch 10/10\n",
      "712/712 [==============================] - 0s - loss: 0.5530 - acc: 0.7612     \n",
      "\n",
      "time taken 0.753010988235 seconds\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "start = time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=6, output_dim=2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print '\\ntime taken %s seconds' % str(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/179 [====>.........................] - ETA: 0s\n",
      "\n",
      "accuracy 0.826815642458\n"
     ]
    }
   ],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print \"\\n\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the loss reduces systematically as the model 'learns' from the data. The rate of loss reduction, however, seems to indicate that loss could be further reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "712/712 [==============================] - 0s - loss: 0.5915 - acc: 0.7514     \n",
      "Epoch 2/10\n",
      "712/712 [==============================] - 0s - loss: 0.5325 - acc: 0.7626     \n",
      "Epoch 3/10\n",
      "712/712 [==============================] - 0s - loss: 0.5006 - acc: 0.7851     \n",
      "Epoch 4/10\n",
      "712/712 [==============================] - 0s - loss: 0.4844 - acc: 0.7893     \n",
      "Epoch 5/10\n",
      "712/712 [==============================] - 0s - loss: 0.4747 - acc: 0.7823     \n",
      "Epoch 6/10\n",
      "712/712 [==============================] - 0s - loss: 0.4691 - acc: 0.7865     \n",
      "Epoch 7/10\n",
      "712/712 [==============================] - 0s - loss: 0.4655 - acc: 0.7865     \n",
      "Epoch 8/10\n",
      "712/712 [==============================] - 0s - loss: 0.4627 - acc: 0.7879     \n",
      "Epoch 9/10\n",
      "712/712 [==============================] - 0s - loss: 0.4606 - acc: 0.7851     \n",
      "Epoch 10/10\n",
      "712/712 [==============================] - 0s - loss: 0.4595 - acc: 0.7893     \n",
      "\n",
      "time taken 0.86990904808 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=6, output_dim=100))\n",
    "model.add(Dense(output_dim=2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print '\\ntime taken %s seconds' % str(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/179 [====>.........................] - ETA: 0s\n",
      "\n",
      "accuracy 0.826815642458\n"
     ]
    }
   ],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print \"\\n\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss reduction 'flattens out' more compared to the 1-layer example. The accuracy, however, remains at 82.7%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "712/712 [==============================] - 0s - loss: 0.5742 - acc: 0.7346     \n",
      "Epoch 2/10\n",
      "712/712 [==============================] - 0s - loss: 0.5061 - acc: 0.7669     \n",
      "Epoch 3/10\n",
      "712/712 [==============================] - 0s - loss: 0.4809 - acc: 0.7781     \n",
      "Epoch 4/10\n",
      "712/712 [==============================] - 0s - loss: 0.4696 - acc: 0.7907     \n",
      "Epoch 5/10\n",
      "712/712 [==============================] - 0s - loss: 0.4646 - acc: 0.7865     \n",
      "Epoch 6/10\n",
      "712/712 [==============================] - 0s - loss: 0.4618 - acc: 0.7865     \n",
      "Epoch 7/10\n",
      "712/712 [==============================] - 0s - loss: 0.4599 - acc: 0.7893     \n",
      "Epoch 8/10\n",
      "712/712 [==============================] - 0s - loss: 0.4591 - acc: 0.7879     \n",
      "Epoch 9/10\n",
      "712/712 [==============================] - 0s - loss: 0.4588 - acc: 0.7865     \n",
      "Epoch 10/10\n",
      "712/712 [==============================] - 0s - loss: 0.4585 - acc: 0.7893     \n",
      "\n",
      "time taken 1.18338799477 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=6, output_dim=100))\n",
    "model.add(Dense(output_dim=100))\n",
    "model.add(Dense(output_dim=2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print '\\ntime taken %s seconds' % str(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/179 [====>.........................] - ETA: 0s\n",
      "\n",
      "accuracy 0.826815642458\n"
     ]
    }
   ],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print \"\\n\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're able to reduce loss on the training set a little further, the accuracy in all three cases is 82.7%. This is due to the fact that the dataset is small, and hence not much for the model to 'learn' from (or for that matter, predict on). We'll apply techniques developed so far on a much larger dataset in the next section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
